{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzihtM4tQrRn",
        "outputId": "f4b2d1cc-3cd0-4b5f-d2f8-d8275dedfee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Train Loss: 2.5234 - Val Loss: 0.2015\n",
            "Epoch 2/50 - Train Loss: 0.1929 - Val Loss: 0.1672\n",
            "Epoch 3/50 - Train Loss: 0.1403 - Val Loss: 0.1173\n",
            "Epoch 4/50 - Train Loss: 0.0976 - Val Loss: 0.0749\n",
            "Epoch 5/50 - Train Loss: 0.0646 - Val Loss: 0.0616\n",
            "Epoch 6/50 - Train Loss: 0.0531 - Val Loss: 0.0485\n",
            "Epoch 7/50 - Train Loss: 0.0382 - Val Loss: 0.0327\n",
            "Epoch 8/50 - Train Loss: 0.0297 - Val Loss: 0.0550\n",
            "Epoch 9/50 - Train Loss: 0.0319 - Val Loss: 0.0245\n",
            "Epoch 10/50 - Train Loss: 0.0193 - Val Loss: 0.0229\n",
            "Epoch 11/50 - Train Loss: 0.0218 - Val Loss: 0.0256\n",
            "Epoch 12/50 - Train Loss: 0.0179 - Val Loss: 0.0222\n",
            "Epoch 13/50 - Train Loss: 0.0165 - Val Loss: 0.0252\n",
            "Epoch 14/50 - Train Loss: 0.0190 - Val Loss: 0.0220\n",
            "Epoch 15/50 - Train Loss: 0.0158 - Val Loss: 0.0225\n",
            "Epoch 16/50 - Train Loss: 0.0159 - Val Loss: 0.0226\n",
            "Epoch 17/50 - Train Loss: 0.0172 - Val Loss: 0.0312\n",
            "Epoch 18/50 - Train Loss: 0.0214 - Val Loss: 0.0542\n",
            "Epoch 19/50 - Train Loss: 0.0247 - Val Loss: 0.0354\n",
            "Epoch 20/50 - Train Loss: 0.0175 - Val Loss: 0.0239\n",
            "Epoch 21/50 - Train Loss: 0.0152 - Val Loss: 0.0225\n",
            "Epoch 22/50 - Train Loss: 0.0147 - Val Loss: 0.0224\n",
            "Epoch 23/50 - Train Loss: 0.0144 - Val Loss: 0.0230\n",
            "Epoch 24/50 - Train Loss: 0.0144 - Val Loss: 0.0219\n",
            "Epoch 25/50 - Train Loss: 0.0140 - Val Loss: 0.0226\n",
            "Epoch 26/50 - Train Loss: 0.0141 - Val Loss: 0.0223\n",
            "Epoch 27/50 - Train Loss: 0.0140 - Val Loss: 0.0227\n",
            "Epoch 28/50 - Train Loss: 0.0135 - Val Loss: 0.0220\n",
            "Epoch 29/50 - Train Loss: 0.0143 - Val Loss: 0.0270\n",
            "Epoch 30/50 - Train Loss: 0.0144 - Val Loss: 0.0221\n",
            "Epoch 31/50 - Train Loss: 0.0136 - Val Loss: 0.0279\n",
            "Epoch 32/50 - Train Loss: 0.0145 - Val Loss: 0.0284\n",
            "Epoch 33/50 - Train Loss: 0.0138 - Val Loss: 0.0220\n",
            "Epoch 34/50 - Train Loss: 0.0134 - Val Loss: 0.0242\n",
            "Epoch 35/50 - Train Loss: 0.0138 - Val Loss: 0.0244\n",
            "Epoch 36/50 - Train Loss: 0.0135 - Val Loss: 0.0225\n",
            "Epoch 37/50 - Train Loss: 0.0129 - Val Loss: 0.0225\n",
            "Epoch 38/50 - Train Loss: 0.0131 - Val Loss: 0.0318\n",
            "Epoch 39/50 - Train Loss: 0.0187 - Val Loss: 0.0286\n",
            "Epoch 40/50 - Train Loss: 0.0141 - Val Loss: 0.0232\n",
            "Epoch 41/50 - Train Loss: 0.0135 - Val Loss: 0.0227\n",
            "Epoch 42/50 - Train Loss: 0.0133 - Val Loss: 0.0230\n",
            "Epoch 43/50 - Train Loss: 0.0131 - Val Loss: 0.0229\n",
            "Epoch 44/50 - Train Loss: 0.0129 - Val Loss: 0.0226\n",
            "Epoch 45/50 - Train Loss: 0.0126 - Val Loss: 0.0222\n",
            "Epoch 46/50 - Train Loss: 0.0125 - Val Loss: 0.0223\n",
            "Epoch 47/50 - Train Loss: 0.0170 - Val Loss: 0.0448\n",
            "Epoch 48/50 - Train Loss: 0.0171 - Val Loss: 0.0347\n",
            "Epoch 49/50 - Train Loss: 0.0146 - Val Loss: 0.0276\n",
            "Epoch 50/50 - Train Loss: 0.0137 - Val Loss: 0.0261\n",
            "‚úÖ Model saved as crop_mlp_model.pth\n",
            "\n",
            "üå± Environment 1: [60, 40, 70, 26, 75, 6.3, 180]\n",
            "Top predicted crops: ['ocotillo_fouquieria', 'pepper_bell', 'okra_clemson', 'cilantro_santo', 'tomato_cherokee', 'pomegranate_wonderful', 'cucumber_persian', 'yucca_soaptree', 'olive_arbequina', 'acacia_small']\n",
            "\n",
            "üå± Environment 2: [10, 5, 5, 20, 60, 5.5, 100]\n",
            "Top predicted crops: ['pineapple', 'banana', 'pepper_jalapeno', 'guava', 'pineapple_smoothcayenne', 'papaya_solo', 'cucumber_armenian', 'opuntia_robusta', 'cocoa_forastero', 'agave_angustifolia']\n",
            "\n",
            "üå± Environment 3: [80, 60, 90, 30, 80, 6.8, 200]\n",
            "Top predicted crops: ['ocotillo_fouquieria', 'pomegranate_wonderful', 'yucca_soaptree', 'okra_clemson', 'pepper_bell', 'cilantro_santo', 'acacia_small', 'mesquite_honey', 'jojoba_simmondsia', 'cucumber_persian']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Load dataset\n",
        "# -----------------------------\n",
        "data = []\n",
        "with open(\"crops.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line.strip()))\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "feature_cols = [\"N\", \"P\", \"K\", \"temperature\", \"humidity\", \"ph\", \"rainfall\"]\n",
        "X = df[feature_cols].values.astype(np.float32)\n",
        "\n",
        "y_raw = df[\"label\"].apply(lambda x: x.split(\",\"))\n",
        "mlb = MultiLabelBinarizer()\n",
        "Y = mlb.fit_transform(y_raw).astype(np.float32)\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Train/Test split\n",
        "# -----------------------------\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.from_numpy(X_train)\n",
        "Y_train_tensor = torch.from_numpy(Y_train)\n",
        "X_val_tensor = torch.from_numpy(X_val)\n",
        "Y_val_tensor = torch.from_numpy(Y_val)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Define MLP Model\n",
        "# -----------------------------\n",
        "class CropMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CropMLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Sigmoid()  # for multi-label classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = Y_train.shape[1]\n",
        "model = CropMLP(input_dim, output_dim)\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Loss and optimizer\n",
        "# -----------------------------\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy for multi-label\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Training loop\n",
        "# -----------------------------\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(xb)\n",
        "        loss = criterion(outputs, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * xb.size(0)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            val_loss += loss.item() * xb.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Save model\n",
        "# -----------------------------\n",
        "torch.save(model.state_dict(), \"crop.pth\")\n",
        "print(\"‚úÖ Model saved as crop_mlp_model.pth\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Prediction function\n",
        "# -----------------------------\n",
        "def predict_crops_nn(model, mlb, env_features, top_n=10):\n",
        "    model.eval()\n",
        "    env_features = torch.tensor(env_features, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        probs = model(env_features).numpy().flatten()\n",
        "    top_indices = probs.argsort()[-top_n:][::-1]\n",
        "    return [mlb.classes_[i] for i in top_indices]\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Test examples\n",
        "# -----------------------------\n",
        "test_envs = [\n",
        "    [60, 40, 70, 26, 75, 6.3, 180],\n",
        "    [10, 5, 5, 20, 60, 5.5, 100],\n",
        "    [80, 60, 90, 30, 80, 6.8, 200]\n",
        "]\n",
        "\n",
        "for i, env in enumerate(test_envs):\n",
        "    top_crops = predict_crops_nn(model, mlb, env, top_n=10)\n",
        "    print(f\"\\nüå± Environment {i+1}: {env}\")\n",
        "    print(\"Top predicted crops:\", top_crops)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNpm-FitSZmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"crop.pth\")\n"
      ],
      "metadata": {
        "id": "M6qSgRm0SeT3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(mlb, \"mlb.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJb7TrveSh8A",
        "outputId": "230f0304-9139-49b8-ced4-cc869ca13445"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mlb.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import joblib\n",
        "\n",
        "# Load mlb\n",
        "mlb = joblib.load(\"mlb.pkl\")\n",
        "\n",
        "# Rebuild model with same dimensions\n",
        "input_dim = 7  # N, P, K, temp, humidity, ph, rainfall\n",
        "output_dim = len(mlb.classes_)\n",
        "\n",
        "class CropMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CropMLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = CropMLP(input_dim, output_dim)\n",
        "model.load_state_dict(torch.load(\"crop.pth\", map_location=torch.device('cpu')))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFigJ--WSpxS",
        "outputId": "15504f3d-3f9d-4356-b76c-26dc931bae13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CropMLP(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=2698, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_crops_nn(model, mlb, env_features, top_n=10):\n",
        "    model.eval()\n",
        "    env_features = torch.tensor(env_features, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        probs = model(env_features).numpy().flatten()\n",
        "    top_indices = probs.argsort()[-top_n:][::-1]\n",
        "    return [mlb.classes_[i] for i in top_indices]\n"
      ],
      "metadata": {
        "id": "fH5oDsD9S3Ho"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from google.colab import files\n",
        "import torch\n",
        "\n",
        "# ‚úÖ Save MultiLabelBinarizer\n",
        "joblib.dump(mlb, 'mlb.pkl')\n",
        "\n",
        "# ‚úÖ Save PyTorch model (state_dict)\n",
        "torch.save(model.state_dict(), 'crop.pth')\n",
        "\n",
        "# ‚úÖ Download both files to local disk\n",
        "files.download('mlb.pkl')\n",
        "files.download('crop.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2VqS3IsfTFwi",
        "outputId": "5510419f-2f02-4021-dae6-931e758ed0b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_14a6106f-05de-4634-b107-cf5e219eb7f5\", \"mlb.pkl\", 51533)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d45c14b-6364-4230-a14d-6cfa0bd70acd\", \"crop.pth\", 2912433)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "5GZVdpbJU6rK",
        "outputId": "3fd7ddf5-e71e-4eb4-db34-bae5969a145a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1216793997.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1216793997.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    WRITE BLOCK CODE TO USE IT PLEASE\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# offline_crop_predict.py\n",
        "# -----------------------------\n",
        "\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Define your MLP model architecture\n",
        "# -----------------------------\n",
        "class CropMLP(nn.Module):\n",
        "    def __init__(self, input_dim=7, output_dim=2698):\n",
        "        super(CropMLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Sigmoid()  # for multi-label classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load saved model and MultiLabelBinarizer\n",
        "# -----------------------------\n",
        "# Load mlb\n",
        "mlb = joblib.load('mlb.pkl')\n",
        "\n",
        "# Load model\n",
        "model = CropMLP(input_dim=7, output_dim=len(mlb.classes_))\n",
        "model.load_state_dict(torch.load('crop.pth', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Prediction function\n",
        "# -----------------------------\n",
        "def predict_crops(env_features, top_n=10):\n",
        "    env_tensor = torch.tensor(env_features, dtype=torch.float32).unsqueeze(0)  # shape: [1, 7]\n",
        "    with torch.no_grad():\n",
        "        probs = model(env_tensor).numpy().flatten()\n",
        "    top_indices = probs.argsort()[-top_n:][::-1]\n",
        "    return [mlb.classes_[i] for i in top_indices]\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Example usage\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    test_envs = [\n",
        "        [60, 40, 70, 26, 75, 6.3, 180],\n",
        "        [10, 5, 5, 20, 60, 5.5, 100],\n",
        "        [80, 60, 90, 30, 80, 6.8, 200]\n",
        "    ]\n",
        "\n",
        "    for i, env in enumerate(test_envs):\n",
        "        top_crops = predict_crops(env, top_n=5)\n",
        "        print(f\"\\nüå± Environment {i+1}: {env}\")\n",
        "        print(\"Top predicted crops:\", top_crops)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-ZgOMwDVJWh",
        "outputId": "3eac90ef-7dea-4e62-85da-a506b240cd76"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üå± Environment 1: [60, 40, 70, 26, 75, 6.3, 180]\n",
            "Top predicted crops: ['ocotillo_fouquieria', 'pepper_bell', 'okra_clemson', 'cilantro_santo', 'tomato_cherokee']\n",
            "\n",
            "üå± Environment 2: [10, 5, 5, 20, 60, 5.5, 100]\n",
            "Top predicted crops: ['pineapple', 'banana', 'pepper_jalapeno', 'guava', 'pineapple_smoothcayenne']\n",
            "\n",
            "üå± Environment 3: [80, 60, 90, 30, 80, 6.8, 200]\n",
            "Top predicted crops: ['ocotillo_fouquieria', 'pomegranate_wonderful', 'yucca_soaptree', 'okra_clemson', 'pepper_bell']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# async_fastapi_crop_api.py\n",
        "# -----------------------------\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import joblib\n",
        "import numpy as np\n",
        "import asyncio\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Define MLP model\n",
        "# -----------------------------\n",
        "class CropMLP(nn.Module):\n",
        "    def __init__(self, input_dim=7, output_dim=2698):\n",
        "        super(CropMLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load saved model and MultiLabelBinarizer\n",
        "# -----------------------------\n",
        "mlb = joblib.load(\"mlb.pkl\")\n",
        "model = CropMLP(input_dim=7, output_dim=len(mlb.classes_))\n",
        "model.load_state_dict(torch.load(\"crop.pth\", map_location=torch.device(\"cpu\")))\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ FastAPI app\n",
        "# -----------------------------\n",
        "app = FastAPI(title=\"Async Crop Prediction API\")\n",
        "\n",
        "# Input schema\n",
        "class Environment(BaseModel):\n",
        "    N: float\n",
        "    P: float\n",
        "    K: float\n",
        "    temperature: float\n",
        "    humidity: float\n",
        "    ph: float\n",
        "    rainfall: float\n",
        "    top_n: int = 5  # optional, default top 5 crops\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Asynchronous prediction function\n",
        "# -----------------------------\n",
        "async def async_predict_crops(env_features, top_n=5):\n",
        "    # Simulate asynchronous behavior for heavy computation\n",
        "    await asyncio.sleep(0)  # yield control to event loop\n",
        "    env_tensor = torch.tensor(env_features, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        probs = model(env_tensor).numpy().flatten()\n",
        "    top_indices = probs.argsort()[-top_n:][::-1]\n",
        "    return [mlb.classes_[i] for i in top_indices]\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Prediction endpoint\n",
        "# -----------------------------\n",
        "@app.post(\"/predict\")\n",
        "async def predict(env: Environment):\n",
        "    features = [\n",
        "        env.N, env.P, env.K,\n",
        "        env.temperature, env.humidity,\n",
        "        env.ph, env.rainfall\n",
        "    ]\n",
        "    crops = await async_predict_crops(features, top_n=env.top_n)\n",
        "    return {\"environment\": features, \"predicted_crops\": crops}\n"
      ],
      "metadata": {
        "id": "dQRwZPlWWiEj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Expose port 8000 (where FastAPI runs)\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"Your public URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "Z7UzB3PWW-aH",
        "outputId": "b05ae058-2bbc-4160-d013-8afce8cbb749"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Downloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-10-07T20:28:13+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-10-07T20:28:13+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-10-07T20:28:13+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-10-07T20:28:13+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1392790023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Expose port 8000 (where FastAPI runs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your public URL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    }
  ]
}